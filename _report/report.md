## 计算方法
总分 = 100 - 场景1的距离 - 场景2的距离 - 场景3的距离
场景i的得分 = 100 - 场景1的距离
# 提交记录
| 提交id | 提交时间 | 总分 |model1方法 | 1得分 | 2得分 |  model1总结  | model2方法 |  3得分 | model2总结 |
|  ----  |  ----  |  ----  | ----  | ----  | ----  | ----  | ---- |---- |---- |
| 1 | | 32.89986638 |官方baseline | 98.79622250 | 49.26791637 | | 用的model1 | 84.83572752 | |
| 2 | | 82.12482187987 | model1置零 | 98.66551103312s | 98.62358332648 | 只用4个通道对场景1的效果影响不大，但是对场景2十分友好| 同上 | 同上| |
| 3 | | 81.81949819408 | 同上 |同上 | 同上 | | 只用1000个标签样本 |  84.53040383447 | 还不如直接用model1 |
| 4 | | 63.62159753349 | 20epoch-->100epoch |85.49242591707 | 86.15884952062 |  100个epoch，model1好像过拟合了，远不如之前20个epoch| 20epoch-->100epoch,最简单的半监督 |   91.97032209580 | 最简单的半监督得到的model2比之前还是有提升的 |
| 5 | 2022/08/07 23:48 | | | | | | 100epoch-->20epoch | 84.94623435439 | 4里面model2训练100epoch比20个epoch效果好，20个epoch欠拟合 |
| 6 | 2022/08/08 16:59 | | | | | | 花时间设计的半监督训练方式  | 91.52268251519 | 比最傻逼的半监督低了0.4个百分点，但是这次所有的训练都只有50个epoch |
| 7 | 2022/08/08 18:29 | 88.49257067111 | 在2的基础上加weight_decay | 98.49197300421 | 98.47791515170 | 加了weight_decay之后略微降了0.2左右 | 同上 | 同上 | 同上 |
| 8 | 2022/08/08 19:30 | | 在7的基础上，20epoch改为40epoch | 98.85236625224 | 98.84850910502 | 可以，是目前最强了 |  |  |  |
| 9 | 2022/08/08 23:16 | | 在7的基础上，L1loss改为MSEloss, 100epoch(重写了model1的网络，按道理是没有变化的) | 99.53934768497 | 99.53123079894 | 目前最强了，继续训或许可以更强 |  |  |  |
| 10-1 | 2022/08/09 00:55 | | 100epoch改为200epoch | 99.61251968129 | 99.60585838940 | 又强了0.1左右 |  |  |  |
| 10-2 | 2022/08/09 09:28 | | 200epoch改为500epoch | 99.68704509416 | 99.67430117920 | 又强了0.1左右,继续训，或许还可以增强 |  |  |  |
| 10-3 | 2022/08/09 13:30 | 92.56184395481 | 500epoch改为1000epoch | 99.71858824222 | 99.71303769030scenario | 又强了0.03左右,继续训，或许还可以增强 |  在4的基础上，傻逼版半监督，打乱数据顺序，1000个epoch |93.13021802229 | 提升了一个百分点 |
| 11 | 2022/08/09 23:11| | 在10-3的基础上打乱数据顺序，10000个epoch，只训到3000多个，最后一次保存是2500epoch | 99.75241118782 | 99.73356698664 | 又好了一些 |   |  |  |
| 12 | 2022/08/11 08:47 | | **限制预测在范围内**，10000个epoch | 99.72530729212 | 99.71998489872 | 从loss来看，没有11下降得快;评测效果略降略降，0.03左右|在6的基础上，改为打标签数量逐渐增多的策略、MSE、weightdecay、随机数种子、加载时打乱数据顺序、每次分数据时也打乱，增加epoch(M0 500，M1M2 500，M_last 1000),限制预测在范围内 | 93.03162626763 | 比6升了1.5,还是不够好 | 
| 13 | 2022/08/11 10:27 | | | | | | 在12的model1的基础上，用1000个有标签数据finetune | 92.44001574439 | 不是很棒 |
|14-2| 2022/08/13 08:50 | | | | | | tri-training | 41.77820351692 | loss下降不太好 |
| 15 | 2022/08/13 09:47 | | | | | | self-training | 93.02363670053 | 每轮20个epoch，loss就下降得可以了，可是效果一般 |
| 16 | 2022/08/15 23:10 | | | | | | 输入幅值，1维卷积 | 96.08785402072 | 评测挺棒，输入幅值挺有用，xm输入幅值到原网络也是96.0 |
| 17-1 | | | | | | |输入幅值，resnet18，后接dropout、非线性、全连接，不加载预训练模型(提交时为防止爆显存，要eval和no_grad)(为了防止联网报错，不要加载预训练模型)，1000epoch | 97.45210068581 | |
| 17-2 | 2022/08/17 10:28 | | | | | | 加载预训练模型，改为4000epoch | 98.14843160368 | 从loss来看，训练集欠拟合了，评测很好，resnet18很有用 |
| 17-3 | | | | | | | 17-2基础上， 全连接层改为512-2 | 97.87331292661 | 从loss也可以看到，过拟合了  |
| 17-5 | | | | | | | 17-3基础上，全连接层512-128-2，dropout为0.1 | 98.00809427331 | 从loss也可以看到，又欠拟合了 | 
| 18-1 | 2022/08/17 17:22 | | | | | |  17-3的基础上，resnet18改为resnet34 |**98.18129227760** | 从loss来看，过拟合了，从测试来看，当前最佳 |
| 18-2 | | | | | | | 18-1的基础上，weight decay改为0.01 |97.74274708161 | 从loss来看，还是过拟合了，训练和测试loss都没18-1好 |
| 18-3 | 2022/08/20 10:35 | | | | | | 18-1的基础上，加dropout=0.3 | 97.98243330091 | 又有点欠拟合了 |
| 18-4 | 2022/08/20 11:20 | | | | | | 18-1的基础上，加dropout=0.1 | 97.93813724976| 训练和验证的loss很均衡，可是评测分数没有体现优势**可能训练和验证没必要均衡，重点是在验证集上要好（即欠拟合，过拟合都行，只要验证集loss低）** |
| 18-5 | 2022/08/20 13:43 | | | | | | 18-1的基础上，加dropout=0.05 | 97.86239343732 |有点过拟合 |
| 19 | 2022/08/19 19:15 | | | | | | 在15 self training的基础上，resnet18改为resnet34，以及输入为幅值，（15与18-1的结合） | 98.18568403176 | 
| 19-2 | 2022/08/19 19:39 | | | | | | 在19的基础上，重复轮数20改为10，epochs20改为100 | 98.23597831945
| 19-3 | 2022/08/19 23:17 | | | | | | 在19的基础上，预训练模型用18-1训练好的模型 | 98.18945133566 |
| 19-4（**还没做**） | | | | | | | 19的基础上，像官方代码一样分一个验证集出来,weight decay
| 20 | | | | | | | 18-1的基础上，从输入三个通道都是一样的幅值，改为，输入两个通道位一样的幅值，第三个通道为相位 | | 从训练和测试loss来看，过拟合很严重，说明相位信息可能没啥用 |
| 21 | | | | | | | 18-1的基础上，resnet34改为resnet50 |  | 无法提交，会爆显存|
| 22 | | | | | | | 17-2的基础上，resnet18改为resnet34 | 98.14994566411 | 也是欠拟合，和17-2效果相近 |
| 23 | | | | | | | 加载22的模型作为预训练模型，self-training，带测试集 | 98.23264712013 | 这种resnet外接全连接层还是老问题，训练loss降不下去，停在1.几，也可能是因为dropout0.3太大了。（没训完，很耗时，只完整地训练7轮，验证集0.7409） |
| 24 | | | 把model1换为23的model2（resnet34外接全连接） | | | 这个时候，model1和model2都这么大的话，提交会爆显存；把model2改为官方的小模型居然还是爆显存，说明这个model1不能这么大;换成17-3中最小的resnet18，也还是爆显存 |
| 25 | | | 在12的基础上，把model1换成16中的幅值、一维卷积 | | | 从loss来看，还不如12中loss下降得快| 
| 26 | | | 在25的基础上，去掉输出限制 | | |  比25，loss下降得更快 |
| 27 | | | 在26的基础上，把模型改为vgg11_bn | | | vgg11_bn有500M+ |
| 28 | |  | 在26的基础上，输如幅值，配合官方原模型 |99.69736780548 | 99.68891700505| 比25，26，loss下降得更快，但好像还是微微不如11，可以试试去掉padding |
| 28-1 | | | 在28的基础上，增加一个全连接层 | 99.73683759550 | 99.71096224744 | 一边训一边提交，可以发现，一般来说，验证集loss越低，测评分数越高；但也不一定，比如5000epoch时验证集loss为0.227低于11的0.295，但是测评分数比11差一些；总共训练大约10000个epoch |
| 28-2 | | | 在28-1的基础上，改schedular  `StepLR(optimizer, step_size=30, gamma=0.5)` | 99.58958395960 | 99.57577081365| lr下降得很快，大约100epoch就乘以0.1； loss下降相比28-1没有优势 |
| 28-3 | | |  在28-1的基础上，改schedular `CosineAnnealingLR(optimizer, T_max=200)` |99.71646299278 | 99.70463546103| 训到3700epoch|
| 29 | | | 在28-2的基础上，增加dropout0.1 | | | 加dropout好像会减缓loss的下降
| 30 | | | 在28-1的基础上，把全连接层再变大`nn.Linear(768*9*4,1000)`,schedular  `StepLR(optimizer, step_size=30, gamma=0.5)`| 99.61008989321 | 99.60330123756 | 这个模型115M了，在改之前模型只有20M，试了一下，不会爆显存，只训了500个epoch|
| 31 | | | 基于30，将输入缩小一倍再输入resnet18（输入幅值，经过二维卷积步长为2，再输入resnet）, batch size 512 |  99.50162316622 | 99.48286763623 | 综合loss和评测分数来看，**貌似欠拟合完全没关系，只要在验证集上表现好即可**|
| 32 | | | 基于31，更改缩小方式（幅值复制三份，经过二维步长为2的池化，再输入resnet）, batch size 256 | 
| 33 | | | 爆显存还有一种解决方式，把不用的维度从取0改为丢弃，那么就不用另外缩小高宽了，batch size 1024（可能太大了点），其他基于32 |
| 34 | | | 基于33，将学习率调整策略改为`ReduceLROnPlateau(optimizer, factor=0.1)`,将batchsize改为32（博客https://www.cnblogs.com/kk17/p/10162510.html说过大batch size不好）| | | **特别注意，这个学习率调整策略的scheduler.step(valloss)这种写法不适用于别的学习率调整策略，记得改过来**|
| 34-1 | | |基于34，factor改为0.5 |
| 34-2 | | |和34-1一样，验证可复现性 **设定可复现，会显著降低训练速度** |
| 35 | | | 基于34 `ReduceLROnPlateau(optimizer, factor=0.8, patience=30,)`  bs 1024| | | loss比34好多了|
| 35-1 | | |基于35，bs 32 | | | 从一两百个epochs来看，训练和验证得loss貌似确实比35（bs 1024）下降地块一些, 但是从长期来看，训练loss下降得也更好，验证loss却下降得很差。**惊奇地发现，35（bs 1024）欠拟合，bs 32过拟,bs512可能会好点？** **貌似小batch size容易过拟合，大batch size容易欠拟合**|
| 35-2 | | |基于35，bs 512 | | | |
| 35-3 | | |基于35，bs 256 |99.79162396550 | 99.77670819206 | 训了6600个epochs（后面几千个epochs没训出啥）|
| 35-3-2 | | |基于35-3，lr 0.01 | | | 应该说没有明显的优势，尽管貌似不那么过拟合|
| 35-4 | | |基于35，bs 128 | | | |
| 35-5 | | |基于35，bs 64 | | | 25-2,35-3,35-4,35-5,占用显存分别为3551M、2339M、1739M、1451M；四者速度上的区别不大，几乎同时开训，花约三个小时分别训到529、521、518、449epoch；四者都是过拟合，从验证集loss来看，batchsize 512和256比较好|
| 36 | | | **transformer** ，model大小只有1.7M | | | loss不太能下降|
| 36-1 | | | 基于36，输入(256,16) | | |  |
| 37-2 | | |基于35-2，resnet18改为resnet34，另外**防爆显存借用了热心小哥哥的策略** | | | |
| 37-3 | | |基于37-2，bs 256 | | | |
| 37-3-2 | | |基于37-3，**seed 42** | | | 貌似影响不大|
| 37-4 | | |基于37-2，bs 128 | | | |
| 37-5 | | |基于37-2，bs 64 | | | |
| 38 | | | | | | |1000个数据，只取四个角的数据，resnet34，bs 32，官方学习率设定| | 从loss来看，不如全用上18个基站的数据|
| 39 | 2022/08/27 23:34 | | | | | |基于38，进行10倍的数据扩增（一部分通道置零），训练集9000个数据, bs 256| 97.96836822572 | 2000epochs |
| 39-（1） | | | | | | | 基于39，不用再训练，预测时采用**TTA** | | |
| 40 | | | | | | |基于39，进行20倍的数据扩增，训练集18000个数据 | 97.68973019731 | 630epochs(老早就训练卡住了)|
| 40-（1） | | | | | | | 基于40，不用再训练，预测时采用TTA | | **后续可以考虑训练的数据扩增和测试时TTA数据扩增，进行一样的数据扩增，而不是随机的**|
| 41 | | | | | | | 变换数据扩增方式，训练集每个样本采用不同的数据扩增形式，训练集10倍数数据扩增 | 98.09202797165 |  800epochs|
| 41-（1） | | | | | | | 41训的模型，在测试时，采用tta | 97.91865551469 | 比41还降了一些|
| 41-2 | | | | | | | 在41的基础上，在训练时，验证集也做数据扩增 | | |
| 41-3 | | | | | | | 在41-2的基础上，把resnet34的全连接层变大，bs128(别人在跑，显存有限) | 97.88956186160 | 1500epochs(后面几百个epochs没变化)|
| 42 | | | | | | | 在41-2的基础上，换为resnet50, bs64(别人在跑，显存有限) |
| 43 | | | 基于35-3，将输入从三份幅值换为实部、虚部、幅值 | 99.83723944491 | 99.83099084369 | 3600epochs，后面几千轮都没变化|
| 43-1 | | | 在43的基础上，resnet18换位renet34 |99.83662511112|99.83289266007| 3780epochs,后面几千轮没有变化 |
| 43-2 | | |  在43-1的基础上，把学习率调整策略改为官方的|
| 43-3 | | | 在43-1的基础上，把学习率调整策略改为`CosineAnnealingLR(optimizer, T_max=100)` |
| 43-4 | | | 在43-1的基础上，把学习率调整策略改为`CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)` | 99.80719578850 | 99.79667816275| 740epochs |
| 44 |  | | | | |  | 在18-1的基础上，将输入从三份幅值换为实部、虚部、幅值，bs改为256 | | |
| 44-1 | | | | | | | 在44的基础上，bs改为32 | **98.62426412664/98.56607091007**| **3000/10000epochs，相比44，验证集loss好了好多好多！貌似只有1000个数据时，batch size 32比较合适** ,10000epochs反而更差？？？？？？？？？？？ |
| 44-1-2 | | | | | | | 在44-1(3000epochs)的基础上，提交时外加输出大小限制| 98.62426412664 | |和不加输出限制时结果一样 |
| 44-2 | | | | | | | 在44的基础上，bs改为16 | 98.22440717360 | 10000epochs相比44-1，loss是差点，但是评测差得有点多|
| 44-3 | | | | | | | 在44-1的基础上，weight decay改为1e-2 | 97.86271751838 | 10000epoch, 与44-1相比，过拟合是好了些，可是验证集loss没有明显优势,验证集loss0.6，可是评分居然这么低|
| 44-4 | | | | | | | 在44-1的基础上，resnet34改为resnet50 |98.41554758651/98.21400689474 | 1000epochs/9000epochs|
| 44-5 | | | | | | | 在44-1的基础上，resnet34改为resnet101 | 98.30293853612 | 5699epochs, model有163M |
| 44-6 | | | | | | | 在44-1的基础上，resnet34改为resnext50_32x4D | 98.26218892736 | model 89M,1390epochs,**相比44-1,loss明明差不多，可是分数却差这么多！！！！！！！，44-这个系列得实验说明在model2上，这个验证集loss的说服力不是很够！！！！！！！！！**|
| 44-7 | | | | | | | 在44-1的基础上，resnet34改为resnext101_32x8d | | model **332M**|
| 44-8 | | | | | | |在44-1的基础上，weight decay改为2e-4 | **98.62426412664**/ 98.56607091007 | 2400epochs/10000epochs |
| 44-9 | | | | | | |在44-1的基础上，weight decay改为1e-3 | 98.45415858159/98.41634253565 | 5600/10000epochs |
| 44-10 | | | | | | |在44-1的基础上，增大全连接层 | | 效果很差|
| 44-11 | | | | | | |在44-1的基础上，weight decay改为5e-4 | 98.43036434774/98.56559304506|550epochs/7545epochs |
| 44-12 | | | | | | |在44-1的基础上，学习率策略改为reducelronpleatu | 98.24390548934| 10000epochs|
| 44-13 | | | | | | |在44-1的基础上，resnet不加载预训练权重 | 98.01732483132| 10000epochs|
| 44-14 | | | | | | |在44-1的基础上，resnet34改为resnet18 | 98.47909717720 | 10000epochs|
| 44-15 | | | | | | |在44-1的基础上，随机数种子改为42 |98.55443826034/98.46493644120 | 894epochs/8926epochs|
| 44-16 | | | | | | |在44-1的基础上, split-ratio改为0.2 | 98.26361826185 | 10000epochs |
| 45| | | 在36的基础上，加上位置编码 |
| 46 | | | 在43-1的基础上，将原始数据扩增一份只有那4个基站的，现在相当于用30000份数据，此时modeldesign2里面就不用消除不用的基站了 |
| 46-1 | | | 46跑一半断电了，在46的基础上，重新跑，另外**关掉了可复现设置** |99.82658409488(769epochs)/**99.86826354375(3616epochs)** | 99.83500137379/**99.88307600161** | |
| 46-2 | | | 在46-1的基础上，将resnet34改为resnext50_32x4d |
| 46-2-1 | | | 46-2中断了，加载中断时的模型继续训练 |
| 46-2-2 | | | 46-2-1中断了，加载中断时的模型继续训练,学习率也从中断的位置继续|
| 46-2-2 | | | 46-2-1中断了，换CosineAnnealingLR继续|
| 47 | | | | | | | 在44-1的基础上，**外加分类loss** |
| 47-1 | | | | | | | 在47的基础上，验证集只用回归损失 | 98.38765104531 | |
| 48 | | | | | | | 在47-1(3042epochs)的基础上（作为预训练模型），**利用分类阈值(设为0.99)self-training 十轮，每轮500epochs**, batch size要改为256 | | loss基本不动|
| 48-1 | | | | | | | 在48的基础上，改为每轮5个epochs.初始学习率改为1e-4 | | loss基本不动|
| 48-2 | | | | | | | 在48的基础上，每次用混合数据是从头开始训,每次训1500epochs | 98.43154673283/98.44188881971 | 第一轮没训完_1036epochs（奇怪，验证loss已经比44-1低很多了，可是测试效果还是没它好,**应该是因为这个预训练模型和这个48-2的随机数不一样，导致这个预训练模型实际上见过不少48-2的验证集数据**）/第一轮没训完_1397epochs。 第二次混合数据训练训了1000轮左右，验证集loss没能比第一次混合数据训练的更低 |
| 48-3 | | | | | | | 在48-2的基础上，不用预训练模型，从零训一个,split ratio 0.3, begin_epochs 5000, epochs 2000 | | split ratio 0.3训出来的初始模型稍微差点| | 混合数据训了两次，验证集loss没有变化|
| 48-4 | | | | | | | 在48-3的基础上，split ratio 0.1, begin_epochs 10000, epochs 2000, **加载分好的数据集（以后就可以都这样，保证模型没有见过测试集）** **self-training慢慢增加数据，每次选置信度最高的数据**|98.38765104531| self-training一点都训不起来|
| 49 |  | | | | | | co-training | | co-trainging没有降低验证loss，导致无效训练| | co训了两轮，差一点的model B在第一轮有进步，model A没进步，第二轮都没有进步 |
| 49-1 |  | | | | | | 49的基础上，begin_epochs 2000 | | co-training第一轮中， modelA和modelB都没有进步，后面不必训了 |
| 49-2 |  | | | | | | 49的基础上，begin_epochs 5000, split ratio 0.5, co_epochs 1000 | | 预训练就差一点，co训了两轮，差一点的model B在第一轮有进步，model A没进步，第二轮都没有进步 |
| 50 |  | | | | | | pytorch里面较强的regnet_y_128gf, batch size 16,再大就爆显存了 |
| 51 |  | | | | | | pytorch里面较强的且大小合适的efficientnet_b6, batch size 32,**之前的模型保存先放到cuda0上改为先放到cpu上** |  **98.66661113552** | **好不容易提交上，**|
| 51-2 |  | | | | | | 模型换为efficientnet_v2_l |
| 52 |  | | | | | | ResNeXt50_32X4D,与44-6的区别是这里加载了V2版本的预训练权重，以及加了分类损失 | 98.29396805500|
| 53 | | | | | | | 加载7934个epochs的51作为教师给无标签数据打标签（只取前面最高的13000个），再教会resnet34 | **98.67820634** | 14000个标签最高置信度为1，平均为0.96，最低为0.27， 第10000个位0.98|
| 53-1 | | | | | | | 基于53，只取前10000个 |  98.46066274398 |826epochs |
| 53-2 | | | | | | | 基于53，取所有14000个 | **98.67194414254/98.66391114192** | 919epochs/3542epochs ，对比53，53-1，53-2可以发现，分类置信度可能没啥用？|
| 54-1 | | | | | | | **bagging** ，**更换验证计算方式为评分那个**|  | |
| 54-2 | | | | | | | 同上|  | |
| 54-3 | | | | | | | 同上|  | |
| 54-4 | | | | | | | 同上|  | |
| 54-5 | | | | | | | 同上| 98.13350565039 | |
| 54-6 | | | | | | | **不加载预训练模型**|  | |
| 54-7 | | | | | | | **不加载预训练模型**|  | |
| 54-8 | | | | | | | **不加载预训练模型**|  | **54-6，7，8与54-1，2，3，4，5相比，从测试集表现来看，不加载预训练模型效果确实差点**|
| 55 | | | | | | |用54-1到54-5做bagging去给无标签数据打标签，然后混合数据去训resnet34，**和bagging一样是用自助法得验证集，可实际上，虚假标签里蕴含了验证集信息，所以可能不太合理，后续可以不分验证集**| 98.33089255216 | 606epochs |
| 55-1 | | | | | | | 在55的基础上，55忘了写更新学习率 |98.39439643453 | |
| 56 | | | | | | | 在44-1的基础上，**不要验证集**，训10000epochs, 每500epochs保存一个模型 | 98.35430387931（10000epochs） | **从之前44系列实验来看，可能不要验证集不太行，因为即使是reducelronpleatu的策略，后面学习率几乎为0，但是验证集的效果还是有很大的波动，根本不知道哪个epochs比较合适**|
| 57-1 | | | | | | | 在54-1的基础上模型换位resnet34|  | |
| 57-2 | | | | | | | weightdecay改为5e-3|  | |
| 57-3 | | | | | | | weightdecay改为1e-3|  | |
| 57-4 | | | | | | | weightdecay改为5e-2|  | |
| 57-5 | | | | | | | weightdecay改为1e-2|  | |
| 58-1 | | | | | | | efficientnet_b6, **1000份数据分为10份，第0份为测试，后9份为训练，测试用的score** |98.18711200482（2056epoch）|实测分数低于训练时的测试分数很多|
| 58-2 | | | | | | | efficientnet_b5,  |
| 58-3 | | | | | | | efficientnet_b4,  |
| 58-4 | | | | | | | efficientnet_b3,  |98.46654472441|（6803epochs）
| 58-5 | | | | | | | efficientnet_b2,  | 98.39759301087（ 4021epochs） |从58-1和58-5的实测分数和训练分数来看，训练分数不是特别能体现实测分数 |
| 58-6 | | | | | | | efficientnet_b1,  |
| 58-7 | | | | | | | resnet34,  |
| 58-8 | | | | | | | resnext50_32x4d,  |
| 58-9 | | | | | | | mnasnet1_3,  |
| 58-10 | | | | | | | shufflenet_v2_x2_0,  |
| 58-11 | | | | | | | densenet201,  |
| 58-12 | | | | | | |  mobilenet_v2,  |
| 58-13 | | | | | | |  inception_v3,  | | 输入尺寸不满足要求|
| 59 |  | | | | | |fusion-label,然后训efficientnet_b6，bs 128（再大就爆显存了） | 
| 59-2 |  | | | | | |基于59，网络换位efficientnet_b3，bs 256，（由于弱模型一直在训练，因此弱模型与59应该有些区别） | 
| 60 | | | 基于46-1，网络换为efficientnet_b3 | |  | 42M，训得太慢了!|
| 61 | | | 基于46-1，网络换为efficientnet_b2 | |  | 30M,也很慢，离谱|