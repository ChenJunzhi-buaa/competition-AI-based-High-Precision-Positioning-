## 计算方法
总分 = 100 - 场景1的距离 - 场景2的距离 - 场景3的距离
场景i的得分 = 100 - 场景1的距离
# 提交记录
| 提交id | 提交时间 | 总分 |model1方法 | 1得分 | 2得分 |  model1总结  | model2方法 |  3得分 | model2总结 |
|  ----  |  ----  |  ----  | ----  | ----  | ----  | ----  | ---- |---- |---- |
| 1 | | 32.89986638 |官方baseline | 98.79622250 | 49.26791637 | | 用的model1 | 84.83572752 | |
| 2 | | 82.12482187987 | model1置零 | 98.66551103312s | 98.62358332648 | 只用4个通道对场景1的效果影响不大，但是对场景2十分友好| 同上 | 同上| |
| 3 | | 81.81949819408 | 同上 |同上 | 同上 | | 只用1000个标签样本 |  84.53040383447 | 还不如直接用model1 |
| 4 | | 63.62159753349 | 20epoch-->100epoch |85.49242591707 | 86.15884952062 |  100个epoch，model1好像过拟合了，远不如之前20个epoch| 20epoch-->100epoch,最简单的半监督 |   91.97032209580 | 最简单的半监督得到的model2比之前还是有提升的 |
| 5 | 2022/08/07 23:48 | | | | | | 100epoch-->20epoch | 84.94623435439 | 4里面model2训练100epoch比20个epoch效果好，20个epoch欠拟合 |
| 6 | 2022/08/08 16:59 | | | | | | 花时间设计的半监督训练方式  | 91.52268251519 | 比最傻逼的半监督低了0.4个百分点，但是这次所有的训练都只有50个epoch |
| 7 | 2022/08/08 18:29 | 88.49257067111 | 在2的基础上加weight_decay | 98.49197300421 | 98.47791515170 | 加了weight_decay之后略微降了0.2左右 | 同上 | 同上 | 同上 |
| 8 | 2022/08/08 19:30 | | 在7的基础上，20epoch改为40epoch | 98.85236625224 | 98.84850910502 | 可以，是目前最强了 |  |  |  |
| 9 | 2022/08/08 23:16 | | 在7的基础上，L1loss改为MSEloss, 100epoch(重写了model1的网络，按道理是没有变化的) | 99.53934768497 | 99.53123079894 | 目前最强了，继续训或许可以更强 |  |  |  |
| 10-1 | 2022/08/09 00:55 | | 100epoch改为200epoch | 99.61251968129 | 99.60585838940 | 又强了0.1左右 |  |  |  |
| 10-2 | 2022/08/09 09:28 | | 200epoch改为500epoch | 99.68704509416 | 99.67430117920 | 又强了0.1左右,继续训，或许还可以增强 |  |  |  |
| 10-3 | 2022/08/09 13:30 | 92.56184395481 | 500epoch改为1000epoch | 99.71858824222 | 99.71303769030scenario | 又强了0.03左右,继续训，或许还可以增强 |  在4的基础上，傻逼版半监督，打乱数据顺序，1000个epoch |93.13021802229 | 提升了一个百分点 |
| 11 | 2022/08/09 23:11| | 在10-3的基础上打乱数据顺序，10000个epoch，只训到3000多个，最后一次保存是2500epoch | 99.75241118782 | 99.73356698664 | 又好了一些 |   |  |  |
| 12 | 2022/08/11 08:47 | | **限制预测在范围内**，10000个epoch | 99.72530729212 | 99.71998489872 | 从loss来看，没有11下降得快;评测效果略降略降，0.03左右|在6的基础上，改为打标签数量逐渐增多的策略、MSE、weightdecay、随机数种子、加载时打乱数据顺序、每次分数据时也打乱，增加epoch(M0 500，M1M2 500，M_last 1000),限制预测在范围内 | 93.03162626763 | 比6升了1.5,还是不够好 | 
| 13 | 2022/08/11 10:27 | | | | | | 在12的model1的基础上，用1000个有标签数据finetune | 92.44001574439 | 不是很棒 |
|14-2| 2022/08/13 08:50 | | | | | | tri-training | 41.77820351692 | loss下降不太好 |
| 15 | 2022/08/13 09:47 | | | | | | self-training | 93.02363670053 | 每轮20个epoch，loss就下降得可以了，可是效果一般 |
| 16 | 2022/08/15 23:10 | | | | | | 输入幅值，1维卷积 | 96.08785402072 | 评测挺棒，输入幅值挺有用，xm输入幅值到原网络也是96.0 |
| 17-1 | | | | | | |输入幅值，resnet18，后接dropout、非线性、全连接，不加载预训练模型(提交时为防止爆显存，要eval和no_grad)(为了防止联网报错，不要加载预训练模型)，1000epoch | 97.45210068581 | |
| 17-2 | 2022/08/17 10:28 | | | | | | 加载预训练模型，改为4000epoch | 98.14843160368 | 从loss来看，训练集欠拟合了，评测很好，resnet18很有用 |
| 17-3 | | | | | | | 17-2基础上， 全连接层改为512-2 | 97.87331292661 | 从loss也可以看到，过拟合了  |
| 17-5 | | | | | | | 17-3基础上，全连接层512-128-2，dropout为0.1 | 98.00809427331 | 从loss也可以看到，又欠拟合了 | 
| 18-1 | 2022/08/17 17:22 | | | | | |  17-3的基础上，resnet18改为resnet34 |98.18129227760 | 从loss来看，过拟合了，从测试来看，当前最佳 |
| 18-2 | | | | | | | 18-1的基础上，weight decay改为0.01 |97.74274708161 | 从loss来看，还是过拟合了，训练和测试loss都没18-1好 |
| 18-3 | 2022/08/20 10:35 | | | | | | 18-1的基础上，加dropout=0.3 | 97.98243330091 | 又有点欠拟合了 |
| 18-4 | 2022/08/20 11:20 | | | | | | 18-1的基础上，加dropout=0.1 | 97.93813724976| 训练和验证的loss很均衡，可是评测分数没有体现优势**可能训练和验证没必要均衡，重点是在验证集上要好（即欠拟合，过拟合都行，只要验证集loss低）** |
| 18-5 | 2022/08/20 13:43 | | | | | | 18-1的基础上，加dropout=0.05 | 97.86239343732 |有点过拟合 |
| 19 | 2022/08/19 19:15 | | | | | | 在15 self training的基础上，resnet18改为resnet34，以及输入为幅值，（15与18-1的结合） | 98.18568403176 | 
| 19-2 | 2022/08/19 19:39 | | | | | | 在19的基础上，重复轮数20改为10，epochs20改为100 | 98.23597831945
| 19-3 | 2022/08/19 23:17 | | | | | | 在19的基础上，18-1训练好的模型 | 98.18945133566 |
| 19-4 | | | | | | | 19的基础上，像官方代码一样分一个验证集出来,weight decay
| 20 | | | | | | | 18-1的基础上，从输入三个通道都是一样的幅值改为，输入两个通道位一样的幅值，第三个通道为相位 | | 从训练和测试loss来看，过拟合很严重，说明相位信息可能没啥用 |
| 21 | | | | | | | 18-1的基础上，resnet34改为resnet50 |  | 无法提交，会爆显存|
| 22 | | | | | | | 17-2的基础上，resnet18改为resnet34 | 98.14994566411 | 也是欠拟合，和17-2效果相近 |
| 23 | | | | | | | 加载22的模型作为预训练模型，self-training，带测试集 | 98.23264712013 | 这种resnet外接全连接层还是老问题，训练loss降不下去，停在1.几，也可能是因为dropout0.3太大了。（没训完，很耗时，只完整地训练7轮，验证集0.7409） |
| 24 | | | 把model1换为23的model2（resnet34外接全连接） | | | 这个时候，model1和model2都这么大的话，提交会爆显存；把model2改为官方的小模型居然还是爆显存，说明这个model1不能这么大;换成17-3中最小的resnet18，也还是爆显存 |
| 25 | | | 在12的基础上，把model1换成16中的幅值、一维卷积 | | | 从loss来看，还不如12中loss下降得快| 
| 26 | | | 在25的基础上，去掉输出限制 | | |  比25，loss下降得更快 |
| 27 | | | 在26的基础上，把模型改为vgg11_bn | | | vgg11_bn有500M+ |
| 28 | |  | 在26的基础上，输如幅值，配合官方原模型 |99.69736780548 | 99.68891700505| 比25，26，loss下降得更快，但好像还是微微不如11，可以试试去掉padding |
| 28-1 | | | 在28的基础上，增加一个全连接层 | 99.73683759550 | 99.71096224744 | 一边训一边提交，可以发现，一般来说，验证集loss越低，测评分数越高；但也不一定，比如5000epoch时验证集loss为0.227低于11的0.295，但是测评分数比11差一些；总共训练大约10000个epoch |
| 28-2 | | | 在28-1的基础上，改schedular  `StepLR(optimizer, step_size=30, gamma=0.5)` | 99.58958395960 | 99.57577081365| lr下降得很快，大约100epoch就乘以0.1； loss下降相比28-1没有优势 |
| 28-3 | | |  在28-1的基础上，改schedular `CosineAnnealingLR(optimizer, T_max=200)` |99.71646299278 | 99.70463546103| 训到3700epoch|
| 29 | | | 在28-2的基础上，增加dropout0.1 | | | 加dropout好像会减缓loss的下降
| 30 | | | 在28-1的基础上，把全连接层再变大`nn.Linear(768*9*4,1000)`,schedular  `StepLR(optimizer, step_size=30, gamma=0.5)`| 99.61008989321 | 99.60330123756 | 这个模型115M了，在改之前模型只有20M，试了一下，不会爆显存，只训了500个epoch|
| 31 | | | 基于30，将输入缩小一倍再输入resnet18（输入幅值，经过二维卷积步长为2，再输入resnet）, batch size 512 |  99.50162316622 | 99.48286763623 | 综合loss和评测分数来看，**貌似欠拟合完全没关系，只要在验证集上表现好即可**|
| 32 | | | 基于31，更改缩小方式（幅值复制三份，经过二维步长为2的池化，再输入resnet）, batch size 256 | 
| 33 | | | 爆显存还有一种解决方式，把不用的维度从取0改为丢弃，那么就不用另外缩小高宽了，batch size 1024（可能太大了点），其他基于32 |
| 34 | | | 基于33，将学习率调整策略改为`ReduceLROnPlateau(optimizer, factor=0.1)`,将batchsize改为32（博客https://www.cnblogs.com/kk17/p/10162510.html说过大batch size不好）| | | **特别注意，这个学习率调整策略的scheduler.step(valloss)这种写法不适用于别的学习率调整策略，记得改过来**|
| 34-1 | | |基于34，factor改为0.5 |
| 34-2 | | |和34-1一样，验证可复现性 **设定可复现，会显著降低训练速度** |
| 35 | | | 基于34 `ReduceLROnPlateau(optimizer, factor=0.8, patience=30,)`  bs 1024| | | loss比34好多了|
| 35-1 | | |基于35，bs 32 | | | 从一两百个epochs来看，训练和验证得loss貌似确实比35（bs 1024）下降地块一些, 但是从长期来看，训练loss下降得也更好，验证loss却下降得很差。**惊奇地发现，35（bs 1024）欠拟合，bs 32过拟,bs512可能会好点？** **貌似小batch size容易过拟合，大batch size容易欠拟合**|
| 35-2 | | |基于35，bs 512 | | | |
| 35-3 | | |基于35，bs 256 | | | |
| 35-3-2 | | |基于35-3，lr 0.01 | | | |
| 35-4 | | |基于35，bs 128 | | | |
| 35-5 | | |基于35，bs 64 | | | 25-2,35-3,35-4,35-5,占用显存分别为3551M、2339M、1739M、1451M；四者速度上的区别不大，几乎同时开训，花约三个小时分别训到529、521、518、449epoch；四者都是过拟合，从验证集loss来看，batchsize 512和256比较好|
| 36 | | | **transformer** ，model大小只有1.7M | | | loss不太能下降|
| 36-1 | | | 基于36，输入(256,16) | | |  |
| 37-2 | | |基于35-2，resnet18改为resnet34，另外**防爆显存借用了热心小哥哥的策略** | | | |
| 37-3 | | |基于37-2，bs 256 | | | |
| 37-3-2 | | |基于37-3，seed 42 | | | |
| 37-4 | | |基于37-2，bs 128 | | | |
| 37-5 | | |基于37-2，bs 64 | | | |
| 38 | | | | | | |1000个数据，只取四个角的数据，resnet34，bs 32，官方学习率设定|
| 39 | | | | | | |基于38，进行10倍的数据扩增（一部分通道置零），训练集9000个数据, bs 256|
| 40 | | | | | | |基于39，进行20倍的数据扩增，训练集18000个数据 |