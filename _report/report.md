# 得分计算方法
总分 = 100 - 场景1的距离 - 场景2的距离 - 场景3的距离
场景i的得分 = 100 - 场景1的距离
# 提交记录
| 提交id  |model1方法 | 1得分 | 2得分 |  model1总结  | model2方法 |  3得分 | model2总结 |
|  ----  |   ----  | ----  | ----  | ----  | ---- |---- |---- |
| 1 |官方baseline | 98.79622250 | 49.26791637 | | 用的model1 | 84.83572752 | |
| 2 | model1置零 | 98.66551103312s | 98.62358332648 | 只用4个通道对场景1的效果影响不大，但是对场景2十分友好| 同上 | 同上| |
| 3 |  同上 |同上 | 同上 | | 只用1000个标签样本 |  84.53040383447 | 还不如直接用model1 |
| 4 |  20epoch-->100epoch |85.49242591707 | 86.15884952062 |  100个epoch，model1好像过拟合了，远不如之前20个epoch| 20epoch-->100epoch,最简单的半监督 |   91.97032209580 | 最简单的半监督得到的model2比之前还是有提升的 |
| 5 |  | | | | 100epoch-->20epoch | 84.94623435439 | 4里面model2训练100epoch比20个epoch效果好，20个epoch欠拟合 |
| 6 |  | | | | 花时间设计的半监督训练方式  | 91.52268251519 | 比最傻逼的半监督低了0.4个百分点，但是这次所有的训练都只有50个epoch |
| 7 |  在2的基础上加weight_decay | 98.49197300421 | 98.47791515170 | 加了weight_decay之后略微降了0.2左右 | 同上 | 同上 | 同上 |
| 8 |  在7的基础上，20epoch改为40epoch | 98.85236625224 | 98.84850910502 | 可以，是目前最强了 |  |  |  |
| 9 | 在7的基础上，L1loss改为MSEloss, 100epoch(重写了model1的网络，按道理是没有变化的) | 99.53934768497 | 99.53123079894 | 目前最强了，继续训或许可以更强 |  |  |  |
| 10-1 | 100epoch改为200epoch | 99.61251968129 | 99.60585838940 | 又强了0.1左右 |  |  |  |
| 10-2 |200epoch改为500epoch | 99.68704509416 | 99.67430117920 | 又强了0.1左右,继续训，或许还可以增强 |  |  |  |
| 10-3 |  500epoch改为1000epoch | 99.71858824222 | 99.71303769030scenario | 又强了0.03左右,继续训，或许还可以增强 |  在4的基础上，傻逼版半监督，打乱数据顺序，1000个epoch |93.13021802229 | 提升了一个百分点 |
| 11 | 在10-3的基础上打乱数据顺序，10000个epoch，只训到3000多个，最后一次保存是2500epoch | 99.75241118782 | 99.73356698664 | 又好了一些 |   |  |  |
| 12 |  **限制预测在范围内**，10000个epoch | 99.72530729212 | 99.71998489872 | 从loss来看，没有11下降得快;评测效果略降略降，0.03左右|在6的基础上，改为打标签数量逐渐增多的策略、MSE、weightdecay、随机数种子、加载时打乱数据顺序、每次分数据时也打乱，增加epoch(M0 500，M1M2 500，M_last 1000),限制预测在范围内 | 93.03162626763 | 比6升了1.5,还是不够好 | 
| 13 | | | | | 在12的model1的基础上，用1000个有标签数据finetune | 92.44001574439 | 不是很棒 |
|14-2| | | | | tri-training | 41.77820351692 | loss下降不太好 |
| 15 | | | | | self-training | 93.02363670053 | 每轮20个epoch，loss就下降得可以了，可是效果一般 |
| 16 | | | | | 输入幅值，1维卷积 | 96.08785402072 | 评测挺棒，输入幅值挺有用，xm输入幅值到原网络也是96.0 |
| 17-1 | | | |输入幅值，resnet18，后接dropout、非线性、全连接，不加载预训练模型(提交时为防止爆显存，要eval和no_grad)(为了防止联网报错，不要加载预训练模型)，1000epoch | 97.45210068581 | |
| 17-2 | | | | | 加载预训练模型，改为4000epoch | 98.14843160368 | 从loss来看，训练集欠拟合了，评测很好，resnet18很有用 |
| 17-3 | | | | | 17-2基础上， 全连接层改为512-2 | 97.87331292661 | 从loss也可以看到，过拟合了  |
| 17-5 | | | | | 17-3基础上，全连接层512-128-2，dropout为0.1 | 98.00809427331 | 从loss也可以看到，又欠拟合了 | 
| 18-1 | | | | |  17-3的基础上，resnet18改为resnet34 |**98.18129227760** | 从loss来看，过拟合了，从测试来看，当前最佳 |
| 18-2 | | | | | 18-1的基础上，weight decay改为0.01 |97.74274708161 | 从loss来看，还是过拟合了，训练和测试loss都没18-1好 |
| 18-3 | | | | | 18-1的基础上，加dropout=0.3 | 97.98243330091 | 又有点欠拟合了 |
| 18-4 | | | | | 18-1的基础上，加dropout=0.1 | 97.93813724976| 训练和验证的loss很均衡，可是评测分数没有体现优势**可能训练和验证没必要均衡，重点是在验证集上要好（即欠拟合，过拟合都行，只要验证集loss低）** |
| 18-5 | | | | | 18-1的基础上，加dropout=0.05 | 97.86239343732 |有点过拟合 |
| 19 | | | | | 在15 self training的基础上，resnet18改为resnet34，以及输入为幅值，（15与18-1的结合） | 98.18568403176 | 
| 19-2 | | | | | 在19的基础上，重复轮数20改为10，epochs20改为100 | 98.23597831945
| 19-3 | | | | | 在19的基础上，预训练模型用18-1训练好的模型 | 98.18945133566 |
| 19-4（**还没做**） || | | | 19的基础上，像官方代码一样分一个验证集出来,weight decay
| 20 | | | | | 18-1的基础上，从输入三个通道都是一样的幅值，改为，输入两个通道位一样的幅值，第三个通道为相位 | | 从训练和测试loss来看，过拟合很严重，说明相位信息可能没啥用 |
| 21 || | | | 18-1的基础上，resnet34改为resnet50 |  | 无法提交，会爆显存|
| 22 || | | | 17-2的基础上，resnet18改为resnet34 | 98.14994566411 | 也是欠拟合，和17-2效果相近 |
| 23 | | | | | 加载22的模型作为预训练模型，self-training，带测试集 | 98.23264712013 | 这种resnet外接全连接层还是老问题，训练loss降不下去，停在1.几，也可能是因为dropout0.3太大了。（没训完，很耗时，只完整地训练7轮，验证集0.7409） |
| 24 |  把model1换为23的model2（resnet34外接全连接） | | | 这个时候，model1和model2都这么大的话，提交会爆显存；把model2改为官方的小模型居然还是爆显存，说明这个model1不能这么大;换成17-3中最小的resnet18，也还是爆显存 |
| 25 |在12的基础上，把model1换成16中的幅值、一维卷积 | | | 从loss来看，还不如12中loss下降得快| 
| 26 | 在25的基础上，去掉输出限制 | | |  比25，loss下降得更快 |
| 27 |在26的基础上，把模型改为vgg11_bn | | | vgg11_bn有500M+ |
| 28 | 在26的基础上，输如幅值，配合官方原模型 |99.69736780548 | 99.68891700505| 比25，26，loss下降得更快，但好像还是微微不如11，可以试试去掉padding |
| 28-1 |在28的基础上，增加一个全连接层 | 99.73683759550 | 99.71096224744 | 一边训一边提交，可以发现，一般来说，验证集loss越低，测评分数越高；但也不一定，比如5000epoch时验证集loss为0.227低于11的0.295，但是测评分数比11差一些；总共训练大约10000个epoch |
| 28-2 | 在28-1的基础上，改schedular  `StepLR(optimizer, step_size=30, gamma=0.5)` | 99.58958395960 | 99.57577081365| lr下降得很快，大约100epoch就乘以0.1； loss下降相比28-1没有优势 |
| 28-3 | 在28-1的基础上，改schedular `CosineAnnealingLR(optimizer, T_max=200)` |99.71646299278 | 99.70463546103| 训到3700epoch|
| 29 |在28-2的基础上，增加dropout0.1 | | | 加dropout好像会减缓loss的下降
| 30 |  在28-1的基础上，把全连接层再变大`nn.Linear(768*9*4,1000)`,schedular  `StepLR(optimizer, step_size=30, gamma=0.5)`| 99.61008989321 | 99.60330123756 | 这个模型115M了，在改之前模型只有20M，试了一下，不会爆显存，只训了500个epoch|
| 31 |  基于30，将输入缩小一倍再输入resnet18（输入幅值，经过二维卷积步长为2，再输入resnet）, batch size 512 |  99.50162316622 | 99.48286763623 | 综合loss和评测分数来看，**貌似欠拟合完全没关系，只要在验证集上表现好即可**|
| 32 | 基于31，更改缩小方式（幅值复制三份，经过二维步长为2的池化，再输入resnet）, batch size 256 | 
| 33 | 爆显存还有一种解决方式，把不用的维度从取0改为丢弃，那么就不用另外缩小高宽了，batch size 1024（可能太大了点），其他基于32 |
| 34 | 基于33，将学习率调整策略改为`ReduceLROnPlateau(optimizer, factor=0.1)`,将batchsize改为32（博客https://www.cnblogs.com/kk17/p/10162510.html说过大batch size不好）| | | **特别注意，这个学习率调整策略的scheduler.step(valloss)这种写法不适用于别的学习率调整策略，记得改过来**|
| 34-1 |基于34，factor改为0.5 |
| 34-2 |和34-1一样，验证可复现性 **设定可复现，会显著降低训练速度** |
| 35 | 基于34 `ReduceLROnPlateau(optimizer, factor=0.8, patience=30,)`  bs 1024| | | loss比34好多了|
| 35-1 |基于35，bs 32 | | | 从一两百个epochs来看，训练和验证得loss貌似确实比35（bs 1024）下降地块一些, 但是从长期来看，训练loss下降得也更好，验证loss却下降得很差。**惊奇地发现，35（bs 1024）欠拟合，bs 32过拟,bs512可能会好点？** **貌似小batch size容易过拟合，大batch size容易欠拟合**|
| 35-2 |基于35，bs 512 | | | |
| 35-3 |基于35，bs 256 |99.79162396550 | 99.77670819206 | 训了6600个epochs（后面几千个epochs没训出啥）|
| 35-3-2 |基于35-3，lr 0.01 | | | 应该说没有明显的优势，尽管貌似不那么过拟合|
| 35-4 |基于35，bs 128 | | | |
| 35-5 |基于35，bs 64 | | | 25-2,35-3,35-4,35-5,占用显存分别为3551M、2339M、1739M、1451M；四者速度上的区别不大，几乎同时开训，花约三个小时分别训到529、521、518、449epoch；四者都是过拟合，从验证集loss来看，batchsize 512和256比较好|
| 36 |**transformer** ，model大小只有1.7M | | | loss不太能下降|
| 36-1 |基于36，输入(256,16) | | |  |
| 37-2 |基于35-2，resnet18改为resnet34，另外**防爆显存借用了热心小哥哥的策略** | | | |
| 37-3 |基于37-2，bs 256 | | | |
| 37-3-2 |基于37-3，**seed 42** | | | 貌似影响不大|
| 37-4 |基于37-2，bs 128 | | | |
| 37-5 |基于37-2，bs 64 | | | |
| 38 | | | | |1000个数据，只取四个角的数据，resnet34，bs 32，官方学习率设定| | 从loss来看，不如全用上18个基站的数据|
| 39 | | | | |基于38，进行10倍的数据扩增（一部分通道置零），训练集9000个数据, bs 256| 97.96836822572 | 2000epochs |
| 39-（1） | | | | | 基于39，不用再训练，预测时采用**TTA** | | |
| 40 | | | | |基于39，进行20倍的数据扩增，训练集18000个数据 | 97.68973019731 | 630epochs(老早就训练卡住了)|
| 40-（1） | | | | | 基于40，不用再训练，预测时采用TTA | | **后续可以考虑训练的数据扩增和测试时TTA数据扩增，进行一样的数据扩增，而不是随机的**|
| 41 | | | | | 变换数据扩增方式，训练集每个样本采用不同的数据扩增形式，训练集10倍数数据扩增 | 98.09202797165 |  800epochs|
| 41-（1） | | | | | 41训的模型，在测试时，采用tta | 97.91865551469 | 比41还降了一些|
| 41-2 | | | | | 在41的基础上，在训练时，验证集也做数据扩增 | | |
| 41-3 | | | | | 在41-2的基础上，把resnet34的全连接层变大，bs128(别人在跑，显存有限) | 97.88956186160 | 1500epochs(后面几百个epochs没变化)|
| 42 | | | | | 在41-2的基础上，换为resnet50, bs64(别人在跑，显存有限) |
| 43 | 基于35-3，将输入从三份幅值换为实部、虚部、幅值 | 99.83723944491 | 99.83099084369 | 3600epochs，后面几千轮都没变化|
| 43-1 | 在43的基础上，resnet18换位renet34 |99.83662511112|99.83289266007| 3780epochs,后面几千轮没有变化 |
| 43-2 | 在43-1的基础上，把学习率调整策略改为官方的|
| 43-3 | 在43-1的基础上，把学习率调整策略改为`CosineAnnealingLR(optimizer, T_max=100)` |
| 43-4 |在43-1的基础上，把学习率调整策略改为`CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)` | 99.80719578850(740epochs)/99.82862255613(3179epochs) | 99.79667816275/ 99.82475376478| |
| 44 | | | |  | 在18-1的基础上，将输入从三份幅值换为实部、虚部、幅值，bs改为256 | | |
| 44-1 | | | | | 在44的基础上，bs改为32 | **98.62426412664/98.56607091007**| **3000/10000epochs，相比44，验证集loss好了好多好多！貌似只有1000个数据时，batch size 32比较合适** ,10000epochs反而更差？？？？？？？？？？？ |
| 44-1-2 | | | | | 在44-1(3000epochs)的基础上，提交时外加输出大小限制| 98.62426412664 | |和不加输出限制时结果一样 |
| 44-2 | | | | | 在44的基础上，bs改为16 | 98.22440717360 | 10000epochs相比44-1，loss是差点，但是评测差得有点多|
| 44-3 | | | | | 在44-1的基础上，weight decay改为1e-2 | 97.86271751838 | 10000epoch, 与44-1相比，过拟合是好了些，可是验证集loss没有明显优势,验证集loss0.6，可是评分居然这么低|
| 44-4 | | | | | 在44-1的基础上，resnet34改为resnet50 |98.41554758651/98.21400689474 | 1000epochs/9000epochs|
| 44-5 | | | | | 在44-1的基础上，resnet34改为resnet101 | 98.30293853612 | 5699epochs, model有163M |
| 44-6 | | | | | 在44-1的基础上，resnet34改为resnext50_32x4D | 98.26218892736 | model 89M,1390epochs,**相比44-1,loss明明差不多，可是分数却差这么多！！！！！！！，44-这个系列得实验说明在model2上，这个验证集loss的说服力不是很够！！！！！！！！！**|
| 44-7 | | | | | 在44-1的基础上，resnet34改为resnext101_32x8d | | model **332M**|
| 44-8 | | | | |在44-1的基础上，weight decay改为2e-4 | **98.62426412664**/ 98.56607091007 | 2400epochs/10000epochs |
| 44-9 | | | | |在44-1的基础上，weight decay改为1e-3 | 98.45415858159/98.41634253565 | 5600/10000epochs |
| 44-10 | | | | |在44-1的基础上，增大全连接层 | | 效果很差|
| 44-11 | | | | |在44-1的基础上，weight decay改为5e-4 | 98.43036434774/98.56559304506|550epochs/7545epochs |
| 44-12 | | | | |在44-1的基础上，学习率策略改为reducelronpleatu | 98.24390548934| 10000epochs|
| 44-13 | | | | |在44-1的基础上，resnet不加载预训练权重 | 98.01732483132| 10000epochs|
| 44-14 | | | | |在44-1的基础上，resnet34改为resnet18 | 98.47909717720 | 10000epochs|
| 44-15 | | | | |在44-1的基础上，随机数种子改为42 |98.55443826034/98.46493644120 | 894epochs/8926epochs|
| 44-16 | | | | |在44-1的基础上, split-ratio改为0.2 | 98.26361826185 | 10000epochs |
| 45|在36的基础上，加上位置编码 |
| 46 |在43-1的基础上，将原始数据扩增一份只有那4个基站的，现在相当于用30000份数据，此时modeldesign2里面就不用消除不用的基站了 |
| 46-1 |46跑一半断电了，在46的基础上，重新跑，另外**关掉了可复现设置** |99.82658409488(769epochs)/**99.86826354375(3616epochs)** | 99.83500137379/**99.88307600161** | |
| 46-2 | 在46-1的基础上，将resnet34改为resnext50_32x4d |
| 46-2-1 |46-2中断了，加载中断时的模型继续训练 |
| 46-2-2 | 46-2-1中断了，加载中断时的模型继续训练,学习率也从中断的位置继续| 99.84897749475 |99.87151723624|
| 46-2-3 |46-2-1中断了，换CosineAnnealingLR继续| 99.82785301600 | 99.83602181480 |
| 47 | | | | | 在44-1的基础上，**外加分类loss** |
| 47-1 | | | | | 在47的基础上，验证集只用回归损失 | 98.38765104531 | |
| 48 | | | | | 在47-1(3042epochs)的基础上（作为预训练模型），**利用分类阈值(设为0.99)self-training 十轮，每轮500epochs**, batch size要改为256 | | loss基本不动|
| 48-1 | | | | | 在48的基础上，改为每轮5个epochs.初始学习率改为1e-4 | | loss基本不动|
| 48-2 | | | | | 在48的基础上，每次用混合数据是从头开始训,每次训1500epochs | 98.43154673283/98.44188881971 | 第一轮没训完_1036epochs（奇怪，验证loss已经比44-1低很多了，可是测试效果还是没它好,**应该是因为这个预训练模型和这个48-2的随机数不一样，导致这个预训练模型实际上见过不少48-2的验证集数据**）/第一轮没训完_1397epochs。 第二次混合数据训练训了1000轮左右，验证集loss没能比第一次混合数据训练的更低 |
| 48-3 | | | | | 在48-2的基础上，不用预训练模型，从零训一个,split ratio 0.3, begin_epochs 5000, epochs 2000 | | split ratio 0.3训出来的初始模型稍微差点| | 混合数据训了两次，验证集loss没有变化|
| 48-4 | | | | | 在48-3的基础上，split ratio 0.1, begin_epochs 10000, epochs 2000, **加载分好的数据集（以后就可以都这样，保证模型没有见过测试集）** **self-training慢慢增加数据，每次选置信度最高的数据**|98.38765104531| self-training一点都训不起来|
| 49 | | | | | co-training | | co-trainging没有降低验证loss，导致无效训练| | co训了两轮，差一点的model B在第一轮有进步，model A没进步，第二轮都没有进步 |
| 49-1 | | | | | 49的基础上，begin_epochs 2000 | | co-training第一轮中， modelA和modelB都没有进步，后面不必训了 |
| 49-2 | | | | | 49的基础上，begin_epochs 5000, split ratio 0.5, co_epochs 1000 | | 预训练就差一点，co训了两轮，差一点的model B在第一轮有进步，model A没进步，第二轮都没有进步 |
| 50 | | | | | pytorch里面较强的regnet_y_128gf, batch size 16,再大就爆显存了 |
| 51 | | | | | pytorch里面较强的且大小合适的efficientnet_b6, batch size 32,**之前的模型保存先放到cuda0上改为先放到cpu上** |  **98.66661113552** | **好不容易提交上，**|
| 51-2 | | | | | 模型换为efficientnet_v2_l |
| 52 | | | | | ResNeXt50_32X4D,与44-6的区别是这里加载了V2版本的预训练权重，以及加了分类损失 | 98.29396805500|
| 53 | | | | | 加载7934个epochs的51作为教师给无标签数据打标签（只取前面最高的13000个），再教会resnet34 | **98.67820634** | 14000个标签最高置信度为1，平均为0.96，最低为0.27， 第10000个位0.98|
| 53-1 | | | | | 基于53，只取前10000个 |  98.46066274398 |826epochs |
| 53-2 | | | | | 基于53，取所有14000个 | **98.67194414254/98.66391114192** | 919epochs/3542epochs ，对比53，53-1，53-2可以发现，分类置信度可能没啥用？|
| 54-1 | | | | | **bagging** ，**更换验证计算方式为评分那个**|  | |
| 54-2 | | | | | 同上|  | |
| 54-3 | | | | | 同上|  | |
| 54-4 | | | | | 同上|  | |
| 54-5 | | | | | 同上| 98.13350565039 | |
| 54-6 | | | | | **不加载预训练模型**|  | |
| 54-7 | | | | | **不加载预训练模型**|  | |
| 54-8 | | | | | **不加载预训练模型**|  | **54-6，7，8与54-1，2，3，4，5相比，从测试集表现来看，不加载预训练模型效果确实差点**|
| 55 | | | | |用54-1到54-5做bagging去给无标签数据打标签，然后混合数据去训resnet34，**和bagging一样是用自助法得验证集，可实际上，虚假标签里蕴含了验证集信息，所以可能不太合理，后续可以不分验证集**| 98.33089255216 | 606epochs |
| 55-1 | | | | | 在55的基础上，55忘了写更新学习率 |98.39439643453 | |
| 56 | | | | | 在44-1的基础上，**不要验证集**，训10000epochs, 每500epochs保存一个模型 | 98.48130272301(5000epochs)/98.35430387931(10000epochs) | **从之前44系列实验来看，可能不要验证集不太行，因为即使是reducelronpleatu的策略，后面学习率几乎为0，但是验证集的效果还是有很大的波动，根本不知道哪个epochs比较合适**|
| 57-1 | | | | | 在54-1的基础上模型换位resnet34|  | |
| 57-2 | | | | | weightdecay改为5e-3|  | |
| 57-3 | | | | | weightdecay改为1e-3|  | |
| 57-4 | | | | | weightdecay改为5e-2|  | |
| 57-5 | | | | | weightdecay改为1e-2|  | |
| 58-1 | | | | | efficientnet_b6, **1000份数据分为10份，第0份为测试，后9份为训练，测试用的score** |98.18711200482（2056epoch）/98.55379208700(10000epochs)|实测分数低于训练时的测试分数很多|
| 58-2 | | | | | efficientnet_b5,  |
| 58-3 | | | | | efficientnet_b4,  |
| 58-4 | | | | | efficientnet_b3,  |98.46654472441|（6803epochs）
| 58-4-2 | | | | |  **改为rlrp学习率策略**  | | |
| 58-5 | | | | | efficientnet_b2,  | 98.39759301087（ 4021epochs） |从58-1和58-5的实测分数和训练分数来看，训练分数不是特别能体现实测分数 |
| 58-6 | | | | | efficientnet_b1,  |
| 58-7 | | | | | resnet34,  |
| 58-8 | | | | | resnext50_32x4d,  |
| 58-9 | | | | | mnasnet1_3,  |
| 58-10 | | | | | shufflenet_v2_x2_0,  |
| 58-11 | | | | | densenet201,  |
| 58-12 | | | | |  mobilenet_v2,  |
| 58-12-2 | | | | |  **改为rlrp学习率策略**  | | |
| 58-13-new | | | | |  convnext_small,  | | **效果很差**|
| 58-14 | | | | |  regnet_y_128gf  | | **当时显存不够大没训起来**|
| 58-15 | | | | |  efficientnet_v2_s  | | |
| 58-16 | | | | |  efficientnet_v2_m  | | |
| 58-17 | | | | |  efficientnet_v2_l  | | |
| 58-18 | | | | |  efficientnet_b7  | | |
| 58-19 | | | | |  mobilenet_v3_large  | | **速度很快，七个小时一万轮**|
| 58-20 | | | | |  regnet_y_3_2gf  | | |
| 58-21 | | | | |  wide_resnet50_2  | | |
| 58-22 | | | | |  resnet50  | | |
| 58-22 | | | | |  resnet152  | | |
| 59-1（原名59） | | | | |fusion-label,弱模型1到12，然后训efficientnet_b6，bs 128（再大就爆显存了） |98.82386188909（627epochs）| 最佳集成测试分数为98.74686431884766
|[2已] 59-4（原名59-2） | | | | |基于59，弱模型1到12，网络换为efficientnet_b3，bs 256，（由于弱模型一直在训练，因此弱模型与59应该有些区别） | **98.85220738909**（628epochs）/**98.87996029798**(807epochs) | 最佳集成测试分数为98.89460754394531,从628epochs和807epochs的提交来看，score验证好像比用test_loader验证要靠谱一些|
| **59-4-4** | | | | |弱模型1-12，15，集成模型59-1，59-4， efficientnet_b3，bs 256，**初版自适应阈值法**|98.73718962645(280epochs)/98.78324410966(507epochs)/98.85091545166(1037epochs)| 最佳集成测试分数为98.91415405273438,**要21261M，有什么办法释放空占着的显存？** **自适应选的阈值为98.49，有点太高了** **训得也太慢了，可以考虑换一个更小得模型**|
| （还没训）59-4-4-4 | | | | | **num_workers从默认的0改为4，看能否提速？？？？？** ,**以及用设置进程来释放显存的写法**, **自适应阈值第二版**，集成弱模型1-12，15-23，不集成集成模型| | 最佳集成测试分数为98.85343933105469|
| [2已]**59-6** | | | | | 59-4-4-4的基础上，集成集成模型59-1，59-4，59-4-4, 训efficientnet_b1|98.66088861368(247epochs)/98.79141449441(314epochs)/98.85448660916(1861epochs) | 最佳集成测试分数为98.9930419921875|
| （还没训）59-12 | | | | | 基于59-6,训mobilenet_v2| | 最佳集成测试分数为应该和59-6一样|
| 59-24 | | | | | 59-4-4-4的基础上，集成集成模型59-1，59-4，59-4-4, 训efficientnet_b0, **不要测试集，全拿来训练**| 98.649(1800epochs)/98.57393593869(2800epochs)/98.73172503683（3600epochs）| 最佳集成测试分数为应该和59-6一样**忘了写学习率更新策略**|
| **59-24-24** | | | | | 59-24基础上，**要测试集**，**学习率策略改为官方的**| 98.85148097192(1960epochs)| 最佳集成测试分数为应该和59-6一样|
| [2已]59-25 | | | | | 59-4-4-4的基础上，集成集成模型59-1，59-4，59-4-4, 59-6,59-24-24, 训resnet18 |**98.90117509282(531epochs)**/98.88146800(3749epochs) | 最佳测试集分数98.99073028564453**好快呀**|
| 59-25-25 | | | | | 59-25的基础上，**测试集中的80个放到训练集**，**那个utils中的打标签优化了一下，weight为0的就不打了** **之前写的那个把程序放进一个进程，这个进程里的脚本好像无法pdb debug** |  98.87185328821（1810epoch），测试分时有99.3的 |
| 59-25-25-25 | | | | |59-25-25的基础上, **测试平均loss最低的模型也保存下来** |98.78624867274（1998epochs，loss_min）/98.88124386832(78624867274（1998epochs，score_max）)
| 59-25-25-25-25 | | | | |59-25-25-25的基础上, **测试集中的50个放到训练集**, **不知道为什么之前完全释放显存的写法不行了** |98.88783683372(1575epochs)
| 59-25-25-25-25-25 | | | | |在59-25-25-25-25的基础上，集成1-12，15-23，101-103，**不集成已有的集成模型**|98.87104425945(1595epochs)
| [2已]59-25-25-25-25-25-25 | | | | |在59-25的基础上，集成1-12，15-23，101-103，**集成所有已有的集成模型(见过100个测试数据的不要)**， **另外不要测试集，那一百个数据复制成14900/9=1600个，即复制到一共16份,bs 1024**,**增大了最佳权重的搜寻范围****发现之前写的集成已有集成模型的代码有点问题，同一个模型的一直集成的是第一个**|98.87695468230(2200epochs)/98.86039737628(2000epochs)/ 98.86210921933(1800epochs)/98.88552945850(1600epochs) |**loss一直到最后没有收敛到0.002这种期望的常规水平！学习率降低策略此时是训练loss30epoch不降就降低学习率**|
| 59-25-25-25-25-25-25-25_ | | | | |在59-25-25-25-25-25-25的基础上，rlrp的patience改为100epochs| ||
| [2已]59-7 | | | | | 59-25的基础上，训resnet34 | 98.87380283811(3736epochs)| |
| 59-20 | | | | | 59-25-25-25-25-25的基础上，训模型regnet_y_3_2gf | | **regnet_y_3_2gf不能提交**|
| (60) |基于46-1，网络换为efficientnet_b3 | |  | 42M，训得太慢了!|
| (61) |基于46-1，网络换为efficientnet_b2 | |  | 30M,也很慢，离谱|
| 61-2 | 61接着训 |99.54089000774(18epochs)/99.65248197702(44epochs) | 99.54863881008/99.58533140576 | **没人和你抢卡，17个小时才训70epochs** |
| (61-4) | 61-3基础上，换为resnet18 | |  | **还是很慢，显存利用率一直接近0** |
| (61-3) | 61-2接着训,**train脚本改为调用utils里面的**, **num_workers改为16(还是这个有效，显存利用率上来了，速度也提升了)** | |  |  |
| (61-3-0) |61-3基础上，num_workers改为0（**dataload默认参数**） | |  | **很慢很慢**|
| (61-3-1) | 61-3基础上，num_workers改为1 | |  | 1分0秒/epoch |
| (61-3-2) | 61-3基础上，num_workers改为2 | |  |  1分1秒/epoch|
| [1已]**61-3-4** | 61-3基础上，num_workers改为4,加载61-2的预训练模型，和学习率，接着训 | 99.7609134932(584epochs) /99.787(1359epochs)| 99.72535347426/99.768 | **1分2秒/epoch**  |
| (61-3-4-2) | 61-3-4基础上，pin_memory改为True | |  | 1分/epoch  |
| (61-3-8) | 61-3基础上，num_workers改为8 | |  | 1分4秒/epoch |
| (61-3-16) | 61-3基础上，num_workers改为16 | |  | 1分6秒/epoch  |
| **61-4** |61-3-4基础上，模型改为resnet18，学习率策略改为SGD，学习率改为0.01 | |  |   |
| [1已]61-5 | 61-3-4基础上，模型改为resnet18，split ratio改为0.05 |99.85319069093(3243epochs) |99.85319069093  |11s/epochs |
| 61-5-5 | 61-5基础上，split ratio改为0.01,**评测时外加输出限制** | |  |  |
| 61-5-5-5(61-5-2-2) | 61-5基础上，bs改为32 | |  |  |
| [1已]61-5-5-5-5(**61-5-2-2-2**) | 61-5基础上，bs改为512 | 99.87930986999(2961epochs)**99.87672947488(2961epochs,minloss)**| 99.89139749225**99.89408065593** | 11s/epochs |
| [1已]61-5-5-5-5-5(61-5-2-2-2-2) |61-5基础上，**bs改为1024** |99.86(1967epochs)/99.876(3466epochs)/99.87920490795(6264epochs)/**99.879(7796epochd)**/差点（10000epochs）|99.86/99.89/99.89/99.90 |14s/epochs(**与61-5，61-5-2-2-2对比发现，bs越大，也不一定更快**) **测试loss最稳定！**  |
| 61-5-5-5-5-5-5(61-5-2-2-2-2-2) | 61-5基础上，bs改为128 ||  | 15s/epochs  |
| [1已]61-5-5-5-5-5-5-5 | 61-5-5-5-5-5-5基础上，因为看最后测试集分数波动很小了，**不用测试集，全拿来训** | 99.85612315435(2400epochs)/99.86963232831(2000epochs)| 99.86554860280/ 99.88154014585| |
| [1已]**61-6** |61-5基础上，模型改为efficientnet_b0 |99.76148569599(1490epochs) | 99.75263203506 | **36s/epochs** |
| 62-3 | | | | |在59-25的基础上，不要已经集成的模型，**只用一半的无标签数据**| ||
| 62-4 | | | | | 基于62-3| ||
| 62-5 | | | | | 基于62-3|98.77683672129（1413epochs） ||
| 62-19 | | | | | 基于62-3| ||
| 62-20 | | | | | 基于62-3| ||
| 62-23 | | | | | 基于62-3| ||
| 62-25 | | | | | 基于62-3| ||
| 63-18 | | | | |在62系列的基础上，不要已经集成的模型，**只用3000的无标签数据**| ||
| 64-25 | | | | |集成所有弱模型，以及62，63系列集成模型，**只用一半的无标签数据** |98.82567605105(651epochs)/98.87308702283(1743epochs) |
| [2已]65-6 | | | | |集成所有没见过100个数据的模型，用3000个无标签数据，模型6，不要测试集，测试集复制到1600了放进了训练集| 98.87235697998(2000epochs) |
| 71-6(2080) | | | | |在61-5的基础上，训练model6 |
| 71-7(2080) | | | | |在61-5的基础上，训练model7,bs128 |
| [1已]111| xm |99.80228581482|99.80127056163|
|_ese-1| 集成了61-5-2-2-2-2-7796epochs，61-3-4，61-6 |99.90171166779 | 99.90324170043 |  |
|_ese-2| 集成了61-5-2-2-2-2-7796epochs，61-3-4，61-6, 61-5-5-5-5-5-5-5-2000epochs |99.91472051654| 99.91898469678 |  |
|_ese-3| 集成了61-5-2-2-2-2-7796epochs，61-3-4，61-6, 61-5-5-5-5-5-5-5-2000epochs, 61-5-2-2-2-2961epochs, 61-5-3423epoch |99.91806494483（阈值99.8）/99.92302159035（阈值99.7）/99.92117082881（阈值99.6）| 99.92324258878/99.92589087688/99.92247904585 |  |
|_ese-4| _ese-3的基础上，**thres从99.7改为99.75** | 比_ese-3还差点 |  |  |
|_ese-5| _ese-3的基础上，自适应阈值找到最佳阈值为99.86000000000013 | 99.91378565452 |99.92188903501  | **还不如_ese-4，说明这种根据已有数据上分数来自适应选阈值的方式不合适** |
|_ese-6| _ese-3的基础上，再集成111，阈值为99.7 |**99.92396721577**  | **99.92708437835** |  |
|_ese-7| _ese-6的基础上，去掉最差的两个子模型2和3 | 99.91901937811 |  99.92509602800|  |
|_ese-8| _ese-6的基础上，去掉模型2 |  |  |  |
|_ese2-1| | |  |  |集成了59-4, 59-6,59-7,59-25-531epochs，阈值为98.8 | **98.90218777155** |
|_ese2-2| | |  |  |集成了59-4, 59-6,59-7,59-25-531epochs，59-25-25-25-25-25-25-1600epochs | 98.90359872946（阈值98.8）/98.90542207385（阈值98.7）/ 98.90534770597（阈值98） |
|_ese2-3| | |  |  |集成了59-4, 59-6,59-7,59-25-531epochs，59-25-25-25-25-25-25-1600epochs, 65-6-2000epochs | **98.90619441723**，又涨了，多集成一点这种见过所有1000个数据的模型还是有好处**同时上交_ese-6和_ese2-3会过大报错，同时上交_ese-8和_ese2-3还是会过大报错（压缩前226不报错，234就报错了）**|
|_ese2-4| | |  |  |基于_ese2-3，去掉最差的59-6 |
|_ese2-5| | |  |  |自适应选的阈值| 98.87993434203 |
